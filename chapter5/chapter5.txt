import org.apache.spark.sql.functions._

/* coalesce shuffle partitions */
spark.conf.get("spark.sql.adaptive.enabled")
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.get("spark.sql.adaptive.enabled")

val dataDF = spark.range(100000000).toDF("pId").withColumn("item_id", when(rand() < 0.8, 100).otherwise(rand() * 3000000).cast("int")) 

dataDF.groupBy("item_id").count().count()



/* persisting data */
import org.apache.spark.sql.functions._
import scala.util.Random

val log_messages = Seq[String](
      "This is a normal line",
      "RuntimeException - this is really bad",
      "ArrayIndexOutOfBoundsException - donâ€™t do this",
      "NullPointerException - this is a nasty one",
      "SQLException - bad SQL again!!!"
)

def getLogMsg(idx:Int) : String = {
	val randomIdx = if (Random.nextFloat() < 0.3) 0 else Random.nextInt(log_messages.size)
	log_messages(randomIdx)
}
val getLogMsgUDF = udf(getLogMsg(_:Int):String)

val app_log_df = spark.range(30000000).toDF("id") .withColumn("msg", getLogMsgUDF(lit(1))) .withColumn("date", date_add(current_timestamp, - (rand() * 360).cast("int")))


val except_log_df = app_log_df.filter('msg.contains("Exception"))

except_log_df.count()

except_log_df.cache()

except_log_df.count()

except_log_df.count()

except_log_df.unpersist()

/* to persist DataFrame with a human readable name */
except_log_df.createOrReplaceTempView("exception_DataFrame")

spark.sqlContext.cacheTable("exception_DataFrame")

spark.sqlContext.uncacheTable("exception_DataFrame")

/* optimize skew join */
import org.apache.spark.sql.functions._


/* broadcast join */
import org.apache.spark.sql.functions._

val states = Seq(("WA", "Washington"), ("CA", "California"), ("AZ", "Arizona"), ("AK", "ALASKA")).toDF("code", "name")

val large_df = spark.range(500000).toDF("id").withColumn("code", when(rand() < 0.2, "WA").when(rand() < 0.4, "CA").when(rand() < 0.6, "AZ").otherwise("AK")).withColumn("date", date_add(current_date, - (rand() * 360).cast("int")))

val joined_df = small_df.join(large_df, "code")

joined_df.explain("extended")

/* shuffle merge sort */
import org.apache.spark.sql.functions._

spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "-1")

val item_df = spark.range(3000000).toDF("item_id").withColumn("price", (rand() * 1000).cast("int"))
val sales_df = spark.range(3000000).toDF("pId").withColumn("item_id", when(rand() < 0.8, 100).otherwise(rand() * 30000000).cast("int")).withColumn("date", date_add(current_date, - (rand() * 360).cast("int")))

val item_sale_df = item_df.join(sales_df, "item_id")

item_sale_df.show()







	